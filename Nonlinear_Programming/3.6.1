Question 3.6.1 (Relation of the Proximal and Gradient Projection Algorithms):

Function: f(x)=(1/2)xQx+bx

Proximal Algorithm: x^{k+1}=argmin{ f(x) +(x-x^k)^2/(2c^k) }

Sequence of positive scalars: {c^k}, such that Q+(1/c^k)I >0 for all k

Constraint Q is symmetric 
     
a) If H^k=Q+(1/c^k)I for s^k=1, alpha^k=1, then
       
       Taylor Expansion:
       
         f(x) = f(x) + f(x)'x + (1/2)*x*x*f(x)'' + O(x^3)

       The books expansion (pg 29 & 32):

         f(x) = f(x)'*x + (1/2)*x*x*f(x)'' + O(x^3) 

       Expansion incorporating x^k:

         f(x) = f(x)'*(x-x^k) + (1/2)*(x-x^k)Q(x-x^k) + f(x^k)'x^k - (1/2)x^k*Q*x^k + O(x^k)

       When Q=H^k-(1/c^k), then:

         f(x) = f(x)'*(x-x^k) + (1/2)*(x-x^k)[H^k-(1/c^k)](x-x^k) + f(x^k)'x^k - (1/2)x^k*[H^k-(1/c^k)]*x^k + ...

              = f(x)*(x-x^k) + (1/2)*(x-x^k)H^k(x-x^k) - (1/2)*(x-x^k)*(x-x^k)/(c^k)- f(x^k)x^k - (1/2)x^k*H^k*x^k + (1/2)x^k*x^k/(c^k) + ...

       Also, delf(x)=0 shows the proximal algorithm and scaled gradient projection method equivalence:

         f(x) = f(x)'*(x-x^k) + (1/2)*(x-x^k)H^k(x-x^k) - (1/2)*(x-x^k)*(x-x^k)/(c^k)- f(x^k)x^k - (1/2)x^k*H^k*x^k + (1/2)x^k*x^k/(c^k) + ...
        
         argmin{f(x)'x^k + (x-x^k)(x-x^k)/(2c^k)} = argmin{f(x)'*(x-x^k) + (1/2)*(x-x^k)H^k(x-x^k) } + ...

                                              = x^{k+1}

b) A generalized algorithm:   x^{k+1} = x^k + alpha^k(xbar^k-x^k)
   
                      where   xbar^k = argmin{ f(x) + (1/2)(x-x^k)M^k(x-x^k) }

                                 M^k + Q > 0

         f(x) = f(x)*(x-x^k) - (1/2)*(x-x^k)M^k(x-x^k) + f(x^k)x^k + (1/2)x^k*M^k*x^k 

       Also, delf(x)=0 shows the proximal algorithm and generalized projection method equivalence:

         f(x) = f(x)*(x-x^k) - (1/2)*(x-x^k)M^k(x-x^k) + f(x^k)x^k + (1/2)x^k*M^k*x^k 
         
         argmin{f(x)x^k + (x-x^k)M^k(x-x^k)} = argmin{f(x)*(x-x^k) + (1/2)*(x-x^k)M^k(x-x^k) }

                                             = argmin{x^k + alpha^k(xbar^k-x^k)}

                                             = x^{k+1}               
c)  When X is real, M^k=Q and alpha^k=2

       Step #0:  x^1 = argmin{x^0 + alpha^0(xbar^1-x^0)}
              
                     = xbar^1
 
       Step #1:  x^2 = argmin{x^1 + alpha^1(xbar^1-x^1)}

                     = 0

Note: An idiosyncratic typeset appeared in the problem with characters "del" and "'" for later review.

Rules: Scaled Gradient Projection Method:
         3.50: x^{k+1} = x^k + alpha^k(xbar^k-x^k)
         3.51: xbar^k = argmin{ f(x)(x-x^k) + (x-x^k)H(x-x^k)/(2s^k) }
